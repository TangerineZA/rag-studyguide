{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama_models\\llama-2-7b-chat.Q5_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (5.52 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4435.49 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1950.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1950.00 MiB, K (f16):  975.00 MiB, V (f16):  975.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.27 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.31 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '16', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama_models\\llama-2-7b-chat.Q5_K_S.gguf\",\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    n_ctx=3900,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make documents from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_paths = os.scandir('resources/all_pdfs/')\n",
    "pages = []\n",
    "for path in pdf_paths:\n",
    "    if path.is_file():\n",
    "        loader = PyPDFLoader('resources/all_pdfs/' + path.name)\n",
    "        new_pages = loader.load_and_split()\n",
    "        pages.extend(new_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Department of Computer Science\\nCOS 132\\nImperative Programming\\nLecturers: Ms. Tayana Morkel, Dr. Patricia Lutu and Dr. Vreda Pieterse\\nc‚ÉùCopyright reserved\\n1' metadata={'source': 'resources/all_pdfs/COS 132 study guide.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate meta-information from first chunk of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "metadata_generation_template = \"\"\"\\\n",
    "<<SYS>>\n",
    "\n",
    "What is the six-character module code for this module in the study guide below? Provide those six characters only - do not provide any other information. No yapping.\n",
    "\n",
    "Format the response as the three letters of the code, and then the three numbers, for example: COS 999\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "[INST]\n",
    "Extract the module code from the below text.\n",
    "\n",
    "study guide snippet: {study_guide_snippet}\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "metadata_generation_prompt_template = PromptTemplate.from_template(metadata_generation_template)\n",
    "\n",
    "metadata_chain = (\n",
    "    {\"study_guide_snippet\": RunnablePassthrough()}\n",
    "    | metadata_generation_prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved data!\n",
      "{'resources/all_pdfs/COS 132 study guide.pdf': 'COS 132', 'resources/all_pdfs/COS110-studyGuide.pdf': 'COS 110', 'resources/all_pdfs/COS122_StudyGuide.pdf': 'COS 122', 'resources/all_pdfs/COS151_study_guide.pdf': 'COS 151', 'resources/all_pdfs/COS212_study_guide.pdf': 'COS 212', 'resources/all_pdfs/COS214StudyGuide2020_V1_0(1).pdf': 'COS 214', 'resources/all_pdfs/COS216_StudyGuide.pdf': 'COS 216', 'resources/all_pdfs/COS710StudyGuide-2023.pdf': 'COS 710', 'resources/all_pdfs/study guide(1).pdf': 'FNAS', 'resources/all_pdfs/Study guide(2).pdf': 'The six-character module code for WTW 148 is: COS 999', 'resources/all_pdfs/study guide.pdf': 'The six-character module code for the module \"Statistics (STK 220)\" in the study guide is:\\nSTK 220', 'resources/all_pdfs/study_guide.pdf': 'COS 999'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "saved_metadata = dict()\n",
    "if os.path.exists('generated_metadata/saved_metadata.pkl'):\n",
    "    with open('generated_metadata/saved_metadata.pkl', 'rb') as f:\n",
    "        saved_metadata = pickle.load(f)\n",
    "        print(\"Loaded saved data!\")\n",
    "        print(saved_metadata)\n",
    "\n",
    "for page in pages:\n",
    "    generated_metadata = \"\"\n",
    "    page.page_content = str(page.page_content).replace('\\n', ' ')\n",
    "    page.page_content = str(page.page_content).replace('.', ' ')\n",
    "    page.page_content = str(page.page_content).replace(\"'\", ' ')  \n",
    "    page.page_content = str(page.page_content).replace(\"`\", ' ')  \n",
    "\n",
    "    # for the \"root\" page of each document\n",
    "    if page.metadata['page'] == 0:\n",
    "        # set generated metadata to either the saved metadata...\n",
    "        if saved_metadata.keys() != None and page.metadata['source'] in saved_metadata.keys():\n",
    "            generated_metadata = saved_metadata[page.metadata['source']]\n",
    "        # or generate new metadata if there is no saved metadata\n",
    "        else:\n",
    "            generated_metadata = metadata_chain.invoke(page.page_content)\n",
    "            saved_metadata[page.metadata['source']] = generated_metadata\n",
    "        \n",
    "        # loop through all documents...\n",
    "for page in pages:\n",
    "    # update content to add metadata context.\n",
    "    page.page_content = 'Module ' + saved_metadata[page.metadata['source']] + ':\\n' + str(page.page_content) + '\\n\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resources/all_pdfs/COS 132 study guide.pdf': 'COS 132',\n",
      " 'resources/all_pdfs/COS110-studyGuide.pdf': 'COS 110',\n",
      " 'resources/all_pdfs/COS122_StudyGuide.pdf': 'COS 122',\n",
      " 'resources/all_pdfs/COS151_study_guide.pdf': 'COS 151',\n",
      " 'resources/all_pdfs/COS212_study_guide.pdf': 'COS 212',\n",
      " 'resources/all_pdfs/COS214StudyGuide2020_V1_0(1).pdf': 'COS 214',\n",
      " 'resources/all_pdfs/COS216_StudyGuide.pdf': 'COS 216',\n",
      " 'resources/all_pdfs/COS710StudyGuide-2023.pdf': 'COS 710',\n",
      " 'resources/all_pdfs/Study guide(2).pdf': 'The six-character module code for '\n",
      "                                          'WTW 148 is: COS 999',\n",
      " 'resources/all_pdfs/study guide(1).pdf': 'FNAS',\n",
      " 'resources/all_pdfs/study guide.pdf': 'The six-character module code for the '\n",
      "                                       'module \"Statistics (STK 220)\" in the '\n",
      "                                       'study guide is:\\n'\n",
      "                                       'STK 220',\n",
      " 'resources/all_pdfs/study_guide.pdf': 'COS 999'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# Prints the nicely formatted dictionary\n",
    "pprint.pprint(saved_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_metadata/saved_metadata.pkl', 'wb') as f:\n",
    "    saved_metadata = pickle.dump(saved_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Module COS 122:\\n'\n",
      " 'The Ô¨Ånal EO is a written exam during the normal examination period  The '\n",
      " 'following regula- tions hold: ‚Ä¢To pass the course, a student must obtain a '\n",
      " 'Ô¨Ånal mark of at least 50%  ‚Ä¢A student will pass the course with distinction '\n",
      " 'if he/she pass the course with a Ô¨Ånal mark of at least 75%  Note that all '\n",
      " 'these mark thresholds are applied strictly  The marks of students who come '\n",
      " 'close to them, but do not actually meet them will notbe condoned (i e  put '\n",
      " 'up)  Please also take note of the examination rules, as provided for in the '\n",
      " 'general rules and regulations of the University of Pretoria, under section G '\n",
      " '12  The date and venue for the exam is scheduled on the ofÔ¨Åcial UP exam time '\n",
      " 'table  6 2 Mark Distribution Marks for all events will be published on the '\n",
      " 'CS Website  Students are responsible for verifying the correctness of these '\n",
      " 'marks  Please Ô¨Åle complaints about marks via email, within 10 calendar days '\n",
      " 'after publication, to the COS122 ticket system  Queries sent directly to '\n",
      " 'lecturers will be ignored  Details about the publication dates and deadlines '\n",
      " 'for complaints are included when marks are published  Continuous assessment '\n",
      " 'will be used and the following table shows how each of the different aspects '\n",
      " 'of your participation contributes to your mark  Activity Weight Assignments '\n",
      " '(3 + 4 + 5 + 6 + 8) 26 Exam Opportunity 1 (ST1) 24 Exam Opportunity 2 (ST2) '\n",
      " '24 Exam Opportunity 3 (Exam) 26 Total 100 Please refer to the examination '\n",
      " 'section on the requirements and restrictions for the exam  6 3 Sick Notes If '\n",
      " 'a student fails to write one of the semester tests a valid sick note may be '\n",
      " 'submitted to the CS department‚Äôs helpdesk within 3 working days after the '\n",
      " 'test to qualify for an aegrotat test  The sick note must be signed by a '\n",
      " 'certiÔ¨Åed doctor  Please tell the staff at the helpdesk to put the note in '\n",
      " 'Nils Timm‚Äôs pigeon hole  If a student fails to write the exam a valid sick '\n",
      " 'note may be submitted to the EBIT Faculty (Eng I level 6) within 3 working '\n",
      " 'days after the exam to qualify for an aegrotat exam  Exam sick notes handed '\n",
      " 'in at the CS department will not be considered as valid documents   6 4 '\n",
      " 'Supplementary Examination Students who do not obtain the sub-minimum of 40% '\n",
      " 'for the exam are not considered for a supplementary examination  7\\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(pages[30].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database from split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(pages, hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a retrieval object which will be used in the query chain to provide context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': 'resources/all_pdfs/COS110-studyGuide.pdf', 'page': 3}, Score: 0.8837088346481323\n",
      "Metadata: {'source': 'resources/all_pdfs/COS 132 study guide.pdf', 'page': 5}, Score: 1.0808874368667603\n",
      "Metadata: {'source': 'resources/all_pdfs/study_guide.pdf', 'page': 7}, Score: 1.205413818359375\n"
     ]
    }
   ],
   "source": [
    "# Sanity check - should return COS122_StudyGuide.pdf\n",
    "test_retrieval_input = \"\"\"\\\n",
    "All practical work must be done in C++ and must compile and run in the Informatorium under\n",
    "Linux, unless explicitly stated otherwise\n",
    "\"\"\"\n",
    "results_with_scores = db.similarity_search_with_score(test_retrieval_input, k=3)\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a template and chain with which to make queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Method for formatting documents to place into the context of a query\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(str(str(doc.metadata) + \":\\n \" + str(doc.page_content) + \"\\n next document:\\n\") for doc in docs)\n",
    "\n",
    "study_guide_rag_template = \"\"\"\\\n",
    "<<SYS>>\n",
    "\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "[INST]\n",
    "question: {question}\n",
    "answer:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(study_guide_rag_template)\n",
    "\n",
    "retrieval_query_template = \"\"\"\\\n",
    "<<SYS>>\n",
    "\n",
    "Extract keywords from the given question.\n",
    "\n",
    "Respond with a comma-seperated list of keywords ONLY. If a module code is found, prioritise it.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "[INST]\n",
    "question: {question}\n",
    "keywords:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "retrieval_query_prompt_template = PromptTemplate.from_template(retrieval_query_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_query_prompt_template | llm | retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the keywords extracted from your question:\n",
      "\n",
      "* Anna Bosman\n",
      "* email address"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1348.50 ms\n",
      "llama_print_timings:      sample time =       3.98 ms /    20 runs   (    0.20 ms per token,  5022.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8312.85 ms /    72 tokens (  115.46 ms per token,     8.66 tokens per second)\n",
      "llama_print_timings:        eval time =    2673.59 ms /    19 runs   (  140.72 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:       total time =   11052.62 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know Anna Bosman's email address as it is not provided in the given documents."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1348.50 ms\n",
      "llama_print_timings:      sample time =       7.49 ms /    23 runs   (    0.33 ms per token,  3069.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  280038.33 ms /  1416 tokens (  197.77 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5915.34 ms /    22 runs   (  268.88 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:       total time =  286393.84 ms /  1438 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "\n",
    "# response = rag_chain.invoke({\"question\": \"What is Anna Bosman's email address?\"}, config={'callbacks': [ConsoleCallbackHandler()]})\n",
    "# response = rag_chain.invoke({\"question\": \"What is Anna Bosman's email address?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know Anna Bosman's email address as it is not provided in the given documents.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a QA Chain from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_response = qa_chain(\"What is Anna Bosman's email address?\")\n",
    "qa_chain_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work:\n",
    "- Implement hybrid search:\n",
    "    - Implement databsae for plaintext storage of document chunks (_db_pt_)\n",
    "    - Implement search algorithm for plaintext through _db_pt_\n",
    "- Test two methods of hybrid search:\n",
    "    - [1] Add data from both search sources\n",
    "    - [2] Use the same document chunks and create a combined metric to choose documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
